% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Introduction}

Here is a test reference~\cite{west_symbolic_2021}.
As the general human knowledge base grows, so does the need of storing it in a format that
is readable and digestible by a computer. As such, knowledge graphs play a very significant role
today in how computer store information and digest it. Generally speaking, knowledge graphs are 
generated by painstaking manual human effort. This has changed with the advent of Large 
Language Models that can now be used to augment currently existing knowledge graphs \cite{west_symbolic_2021}


\section{Knowledge Graphs}\label{sec:knowledgeGraphs}

There are multiple definitions of knowledge graphs. The widely known one comes from Google's 
popularization of the word in 2012 \cite{noauthor_introducing_2012} where the authors imply 
that the knowledge that Google contains is accessible via the Google knowledge graph. On the 
flip side, authors describe knowledge graphs as RDF graphs (a set of RDF triples) \cite{farber_linked_2017}. 
For our purposes, we will describe knowledge graphs as a set of RDF triples that contain 
a subject, a property and an object. 

\section{Commmon Sense Knowledge Graphs}\label{sec:commonsenseKnowledgeGraphs}

Commonsense knowledge plays a crucial role today in many machine learning applications including
natural language processing and computer vision. Commonsense is often provided via a number 
of sources dependening on the application. In order to provide a common source that can play
multiple roles, CommonSense Knowledge Graphs (CSKGs) were born \cite{ilievski_cskg_2021}.

\section{Large Language Models}\label{sec:largeLanguageModel}

Language models, in essense, are probablity distributions over sequences of words \cite{jurafsky_speech_2009}. 
They are used for a variety of purposes that range from the simple such as tab completion to 
sophisticated text generation and reviewing human written translations. 
With the advent of larger and more sophisticated language models such as GPT-3\cite{brown_language_2020}, the scope
of usefuless for language models has expanded significantly. One such use is in the 
augmentation of currently existing commonsense knowledge graphs \cite{west_symbolic_2021}.
This kind of usage requires that large language models (LLMs) such as GPT-2 be trained on the task
of knowledge generation. As per \citeauthor{west_symbolic_2021}, language models fail to express common 
sense knowledge when prompted in a zero-shot manner. As such, the authors converted the models
to COMET models by training them on a knowledge graph. We suspect, such training, while providing 
additional capabilities for knowledge graph generation, reduces other language modelling capabilities
of the trained model. 

\end{document}