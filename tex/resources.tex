% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Resources}\label{sec:resources} 


\section{Models and Tokenizers}\label{sec:models}

The models used in this project come from two different sources. The GPT2-XL model is based on the OpenAI's GPT2 model 
\cite{radford_language_nodate}. The GPT2-XL model has 48 attention modules as compared to the 12 found in the base 
GPT2 model. This gives the GPT2-XL model more capability in terms of language modelling tasks but also requires 
significantly more computing resources to train and work with \cite{noauthor_pretrained_nodate}. This translates to 117 
million parameters in the GPT2 model vs 1.6 billion parameters in the GPT2-XL model. Given that the COMET-DISTIL models
are based on the GPT2-XL model, we opted to utilize the naive GPT2-Xl model as our baseline for comparison. We 
downloaded the GPT2-Xl model by using the Hugginface model library \cite{noauthor_gpt2-xl_nodate}. As our tokenizer, we 
used the pre-trained tokenzier also available from the hugginsface model library \cite{noauthor_gpt2-xl_nodate}. 

The second model we used was the COMET-DISTIL model \cite{west_symbolic_2021} that was trained on the ATOMIC-10X 
knowledge graph. The COMET-DISTIL model was downloaded from the author's provided website that can be found on their 
githhub page \cite{peterwestai2_symbolic_2022}. The tokenizer used with this model was also downloaded from the same 
source. The tokenizers for both models were adjusted to add EOS tokens to ensure that they could work with the datasets 
that were used for evaluationg the models. 

In order to train and test the models, both models had different language modelling heads (dependening on the task) 
connected to the models. The heads were then trained on the training set of the datasets and then evaulted on 
the evaulation set of the datasets. 

\section{Datasets}\label{sec:datasets}
In order to test the performances of the respective models, several different standard datasets were used. This 
included the widely recognized Glue dataset\cite{}. The second benchmark was based on the 
CommonsenseQA dataset\cite{talmor_commonsenseqa_2018} that tests a model's ability to answer a question based on prior 
knowledge. The SuperGLUE benchmark on the other hand is made up of several different datasets that test a model on a 
variety of tasks including BoolQ, CB, COPA and MultiRC.

These two benchmarks were choosen based on their wide variety of tasks and the difficulty 
associated with each of these tasks. In addition, the original glue benchmark was used to test the performance of 
models to add an additional comparison point to the study. 


\section{Compute Resources}\label{sec:computeResources}
To train and evaluate the performance of the respective models, Lambda Compute Instances at the University of Alberta 
were utilized. The machines included two Nvidia N100 Gpus with 24GB of memory. Due to the large size of the GPT2-XL 
Model (5.2 Billion Parameters), the DeepSpeed\cite{rajbhandari_zero_2020} library from Microsoft was also utilized to 
help fit the models within the GPU memory. 

\section{Development Environment}
\begin{enumerate}
    \item Python 3.8.13
    \item Conda 4.11.0 (Environemnt Manager)
    \item jupyterlab 3.3.2 (IDE / Notebook)
    \item pytorch 1.11.0 (Deep Learning Framework)
    \item transformers 4.27.0.dev0 (Huggingface Library)
    \item deepspeed 0.8.1 (Model Training Utility)
\end{enumerate}


\section{Python, JupyterLab, and Conda}\label{sec:python}

We utilized a combination of tools and environments that are foundational in the modern data science and machine 
learning workflows: Python, JupyterLab, and Conda to conduct these experiments. 

\begin{enumerate}
    \item \textbf{Python}: Python has become the backbone of the modern datascience and machine learning 
    world due to its ease of use and the widely supported libraries built on top such as Tensorflow, Pandas and Pytorch. 
    In addition, it is also highly performant as it is able to utilize bindings with other low level languages
    such as C/C++ for intensive computing tasks. This is essentially how numpy is able to do maths so quickly! 
    
    \item \textbf{JupyterLab}: An interactive development environment that provides a platform for writing 
    and executing code, visualizing data. It's an evolution of the original Jupyter notebooks. 
    
    \item \textbf{Conda}: An open-source package management system and also an environment 
    management system. Conda makes it easy to install multiple versions of software packages 
    and their dependencies in isolated environments. This ensures consistent and reproducible 
    results across different stages of a project and across different team members' setups.
\end{enumerate}


\section{Pytorch}\label{sec:pytorch}
PyTorch\cite{paszke_pytorch_2019} is one of the most well-known Deep Learning frameworks. Developed orgiinally by the Facebook's Research Lab (FAIR), 
it is currently an integral part of the huggingface library which was also utilized for this project. It is used both by
researchers and developers. It offers several advantages which allow us to train our models in an effective manner even when 
facing low memory constraints. This is especially important when dealing with large language models such as GPT2-xl. 

Some of the offerings that come with pytorch are it's Dynamic Computation Graph which allow us to change the model architecture 
on the fly. Another advantage is the rich ecosystem of libraries that either support or utilize pytorch such as 
huggingface and deepspeed which we will also utilize. 

\section{Huggingface}\label{sec:huggingface}
Another key piece of software that was utilized to perform these experiments was the Huggingface\cite{wolf_transformers_2020} library. It is 
an open source framework that provides many state-of-the-art implementsions of NLP models, particulary transformer-based models such as BERT, GTP-2 
and GPT-2-xl. Huggingface 

\begin{enumerate}
    \item \textbf{Pre-trained Models}: The library boasts a broad selection of pre-trained models. This feature significantly eases the 
    process of fine-tuning models for specific tasks, removing the necessity to train extensive models from the ground up. We capitalized on this 
    by utilizing the GPT-2-xl models available from the model hub. 
    
    \item \textbf{Flexibility}: While the Transformers library seamlessly integrates with popular deep learning frameworks like 
    PyTorch and TensorFlow, users have the freedom to select their preferred backend. In addition huggingface offers native integrations with 
    Zero and Deepspeed libraries making model training possible even with limited hardware resources. 
    
    \item \textbf{User-friendly API}: The design of the API is intuitive, ensuring that tasks such as tokenization, 
    training, and inference are both streamlined and accessible with minimal coding. This user-centric design approach caters to 
    both novices and experts in the field of NLP. The library also comes with a very hand run\_glue script that we were able to modify to 
    run our experiments for the glue dataset.

\end{enumerate}


\section{Deepspee and ZeRO}\label{sec:deepspeed}

DeepSpeed\cite{rajbhandari_zero_2020} is an open-source deep learning optimization library that offers several techniques for distributed 
training and model parallelism. One of its most renowned components is ZeRO (Zero Redundancy Optimizer), a memory optimization technology.

\begin{enumerate}
    \item \textbf{Scalability}: DeepSpeed drastically improves the scale and speed of training. It can support 
    model sizes of over 100 billion parameters on current generation hardware.
    
    \item \textbf{ZeRO (Zero Redundancy Optimizer)}: A key innovation of DeepSpeed, ZeRO optimizes the memory utilization across 
    distributed model training. By reducing memory redundancy across the data parallelism stages, ZeRO enables the training of models 
    that are up to 10x larger without any significant increase in computational resources.
    
    \item \textbf{Memory Efficiency}: Through its optimization techniques, such as activation checkpointing, 
    DeepSpeed allows for more significant model sizes and batch sizes without the need for proportional increases in memory.
    
    \item \textbf{Integration}: DeepSpeed is designed to be easily integrated into existing PyTorch models, making it a 
    versatile choice for many deep learning applications.
\end{enumerate}

In this report, DeepSpeed and ZeRO were leveraged to train larger models more efficiently, leading to reduced training times and 
enhanced performance on our tasks.

\section{Weights \& Biases}\label{sec:weightsAndBiases}

We also utilized Weights \& Biases (W\&B)\cite{wandb}, a platform designed to help teams track their machine learning experiments, 
visualize metrics, and share findings. We will link our project in the append of this report to allow you to view the raw data from the 
experiments directly. Here are some of the reasons why we relied on Weights \& Biases for tracking our experiments: 

\begin{enumerate}
    \item \textbf{Experiment Tracking}: W\&B provides comprehensive logging of hyperparameters, outputs, code, and metrics. 
    By having a centralized record, researchers and developers can easily compare different runs and iterations, 
    making it simpler to iterate over model architectures and parameters.
    
    \item \textbf{Visualization}: Through interactive charts and graphs, the platform facilitates a deeper understanding 
    of model behavior. This aids in identifying patterns, anomalies, or areas of improvement in model training and validation.
    
    \item \textbf{Reproducibility}: The platform ensures that every detail of the experiment, from code to configurations, 
    is logged. This is crucial for both verifying results and building upon previous work without redundancy.
    
    \item \textbf{Collaboration}: W\&B promotes collaboration by allowing teams to share findings, make comments, and 
    build on each other's work seamlessly. This feature is especially beneficial for distributed teams or projects with 
    multiple stakeholders. This allowed our team to collaborate and share findings on this project as we were all 
    located remotely. 
    
\end{enumerate}

For the experiments detailed in this report, Weights \& Biases was employed to monitor the training process, analyze results, 
and ensure reproducibility across different runs.


\end{document}