
@article{west_symbolic_2021,
	title = {Symbolic {Knowledge} {Distillation}: from {General} {Language} {Models} to {Commonsense} {Models}},
	shorttitle = {Symbolic {Knowledge} {Distillation}},
	url = {https://arxiv.org/abs/2110.07178v1},
	doi = {10.48550/arXiv.2110.07178},
	abstract = {The common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.},
	language = {en},
	urldate = {2022-06-08},
	author = {West, Peter and Bhagavatula, Chandra and Hessel, Jack and Hwang, Jena D. and Jiang, Liwei and Bras, Ronan Le and Lu, Ximing and Welleck, Sean and Choi, Yejin},
	month = oct,
	year = {2021},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/FHMNLHGU/West et al. - 2021 - Symbolic Knowledge Distillation from General Lang.pdf:application/pdf;Snapshot:/Users/sharyarmemon/Zotero/storage/Z7IKV838/2110.html:text/html},
}

@article{hwang_comet-atomic_2020,
	title = {{COMET}-{ATOMIC} 2020: {On} {Symbolic} and {Neural} {Commonsense} {Knowledge} {Graphs}},
	shorttitle = {{COMET}-{ATOMIC} 2020},
	url = {https://arxiv.org/abs/2010.05953v2},
	doi = {10.48550/arXiv.2010.05953},
	abstract = {Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge. In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them. With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that ATOMIC 2020 is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 (175B parameters), while impressive, remains {\textasciitilde}12 absolute points lower than a BART-based knowledge model trained on ATOMIC 2020 despite using over 430x fewer parameters.},
	language = {en},
	urldate = {2022-06-08},
	author = {Hwang, Jena D. and Bhagavatula, Chandra and Bras, Ronan Le and Da, Jeff and Sakaguchi, Keisuke and Bosselut, Antoine and Choi, Yejin},
	month = oct,
	year = {2020},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/HETJTUCD/Hwang et al. - 2020 - COMET-ATOMIC 2020 On Symbolic and Neural Commonse.pdf:application/pdf;Snapshot:/Users/sharyarmemon/Zotero/storage/XPH63D2M/2010.html:text/html},
}

@article{sap_atomic_2018,
	title = {{ATOMIC}: {An} {Atlas} of {Machine} {Commonsense} for {If}-{Then} {Reasoning}},
	shorttitle = {{ATOMIC}},
	url = {https://arxiv.org/abs/1811.00146v3},
	doi = {10.48550/arXiv.1811.00146},
	abstract = {We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., "if X pays Y a compliment, then Y will likely return the compliment"). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.},
	language = {en},
	urldate = {2022-06-08},
	author = {Sap, Maarten and LeBras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A. and Choi, Yejin},
	month = oct,
	year = {2018},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/YQN7KZ9H/Sap et al. - 2018 - ATOMIC An Atlas of Machine Commonsense for If-The.pdf:application/pdf;Snapshot:/Users/sharyarmemon/Zotero/storage/T7Y9HV4Y/1811.html:text/html},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	shorttitle = {Transformers},
	url = {https://aclanthology.org/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2022-06-08},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	month = oct,
	year = {2020},
	pages = {38--45},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/Q3ACVWFN/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf:application/pdf},
}

@inproceedings{ehrlinger_towards_2016,
	title = {Towards a {Definition} of {Knowledge} {Graphs}},
	abstract = {Recently, the term knowledge graph has been used frequently in research and business, usually in close association with Semantic Web technologies, linked data, large-scale data analytics and cloud computing. Its popularity is clearly influenced by the introduction of Google's Knowledge Graph in 2012, and since then the term has been widely used without a definition. A large variety of interpretations has hampered the evolution of a common understanding of knowledge graphs. Numerous research papers refer to Google's Knowledge Graph, although no official documentation about the used methods exists. The prerequisite for widespread academic and commercial adoption of a concept or technology is a common understanding, based ideally on a definition that is free from ambiguity. We tackle this issue by discussing and defining the term knowledge graph, considering its history and diversity in interpretations and use. Our goal is to propose a definition of knowledge graphs that serves as basis for discussions on this topic and contributes to a common vision.},
	author = {Ehrlinger, Lisa and Wöß, Wolfram},
	month = sep,
	year = {2016},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/DL42QWUU/Ehrlinger and Wöß - 2016 - Towards a Definition of Knowledge Graphs.pdf:application/pdf},
}

@misc{noauthor_introducing_2012,
	title = {Introducing the {Knowledge} {Graph}: things, not strings},
	shorttitle = {Introducing the {Knowledge} {Graph}},
	url = {https://blog.google/products/search/introducing-knowledge-graph-things-not/},
	abstract = {We hope this will give you a more complete picture of your interest, provide smarter search results, and pique your curiosity.},
	language = {en-us},
	urldate = {2022-06-08},
	journal = {Google},
	month = may,
	year = {2012},
	file = {Snapshot:/Users/sharyarmemon/Zotero/storage/U7HJHIBY/introducing-knowledge-graph-things-not.html:text/html},
}

@article{farber_linked_2017,
	title = {Linked data quality of {DBpedia}, {Freebase}, {OpenCyc}, {Wikidata}, and {YAGO}},
	volume = {9},
	issn = {22104968, 15700844},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-170275},
	doi = {10.3233/SW-170275},
	abstract = {In recent years, several noteworthy large, cross-domain, and openly available knowledge graphs (KGs) have been created. These include DBpedia, Freebase, OpenCyc, Wikidata, and YAGO. Although extensively in use, these KGs have not been subject to an in-depth comparison so far. In this survey, we provide data quality criteria according to which KGs can be analyzed and analyze and compare the above mentioned KGs. Furthermore, we propose a framework for ﬁnding the most suitable KG for a given setting.},
	language = {en},
	number = {1},
	urldate = {2022-06-08},
	journal = {Semantic Web},
	author = {Färber, Michael and Bartscherer, Frederic and Menne, Carsten and Rettinger, Achim},
	editor = {Zaveri, Amrapali and Kontokostas, Dimitris and Hellmann, Sebastian and Umbrich, Jürgen and Zaveri, Amrapali and Kontokostas, Dimitris and Hellmann, Sebastian and Umbrich, Jürgen},
	month = nov,
	year = {2017},
	pages = {77--129},
	file = {Färber et al. - 2017 - Linked data quality of DBpedia, Freebase, OpenCyc,.pdf:/Users/sharyarmemon/Zotero/storage/PMRYAI6N/Färber et al. - 2017 - Linked data quality of DBpedia, Freebase, OpenCyc,.pdf:application/pdf},
}

@techreport{ilievski_cskg_2021,
	title = {{CSKG}: {The} {CommonSense} {Knowledge} {Graph}},
	shorttitle = {{CSKG}},
	url = {http://arxiv.org/abs/2012.11490},
	abstract = {Sources of commonsense knowledge support applications in natural language understanding, computer vision, and knowledge graphs. Given their complementarity, their integration is desired. Yet, their different foci, modeling approaches, and sparse overlap make integration difficult. In this paper, we consolidate commonsense knowledge by following five principles, which we apply to combine seven key sources into a first integrated CommonSense Knowledge Graph (CSKG). We analyze CSKG and its various text and graph embeddings, showing that CSKG is well-connected and that its embeddings provide a useful entry point to the graph. We demonstrate how CSKG can provide evidence for generalizable downstream reasoning and for pre-training of language models. CSKG and all its embeddings are made publicly available to support further research on commonsense knowledge integration and reasoning.},
	number = {arXiv:2012.11490},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Ilievski, Filip and Szekely, Pedro and Zhang, Bin},
	month = mar,
	year = {2021},
	doi = {10.48550/arXiv.2012.11490},
	note = {arXiv:2012.11490 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Social and Information Networks},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:2006.06114},
	file = {arXiv Fulltext PDF:/Users/sharyarmemon/Zotero/storage/532TLKIY/Ilievski et al. - 2021 - CSKG The CommonSense Knowledge Graph.pdf:application/pdf;arXiv.org Snapshot:/Users/sharyarmemon/Zotero/storage/WS4BSLB4/2012.html:text/html},
}

@techreport{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	number = {arXiv:2005.14165},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	doi = {10.48550/arXiv.2005.14165},
	note = {arXiv:2005.14165 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:/Users/sharyarmemon/Zotero/storage/ZXMZ5WPG/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/sharyarmemon/Zotero/storage/GI6KH8SD/2005.html:text/html},
}

@book{jurafsky_speech_2009,
	address = {USA},
	title = {Speech and {Language} {Processing} (2nd {Edition})},
	isbn = {978-0-13-187321-6},
	publisher = {Prentice-Hall, Inc.},
	author = {Jurafsky, Daniel and Martin, James H.},
	year = {2009},
}
