
@article{west_symbolic_2021,
	title = {Symbolic {Knowledge} {Distillation}: from {General} {Language} {Models} to {Commonsense} {Models}},
	shorttitle = {Symbolic {Knowledge} {Distillation}},
	url = {https://arxiv.org/abs/2110.07178v1},
	doi = {10.48550/arXiv.2110.07178},
	abstract = {The common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.},
	language = {en},
	urldate = {2022-06-08},
	author = {West, Peter and Bhagavatula, Chandra and Hessel, Jack and Hwang, Jena D. and Jiang, Liwei and Bras, Ronan Le and Lu, Ximing and Welleck, Sean and Choi, Yejin},
	month = oct,
	year = {2021},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/FHMNLHGU/West et al. - 2021 - Symbolic Knowledge Distillation from General Lang.pdf:application/pdf;Snapshot:/Users/sharyarmemon/Zotero/storage/Z7IKV838/2110.html:text/html},
}

@article{hwang_comet-atomic_2020,
	title = {{COMET}-{ATOMIC} 2020: {On} {Symbolic} and {Neural} {Commonsense} {Knowledge} {Graphs}},
	shorttitle = {{COMET}-{ATOMIC} 2020},
	url = {https://arxiv.org/abs/2010.05953v2},
	doi = {10.48550/arXiv.2010.05953},
	abstract = {Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge. In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them. With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that ATOMIC 2020 is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 (175B parameters), while impressive, remains {\textasciitilde}12 absolute points lower than a BART-based knowledge model trained on ATOMIC 2020 despite using over 430x fewer parameters.},
	language = {en},
	urldate = {2022-06-08},
	author = {Hwang, Jena D. and Bhagavatula, Chandra and Bras, Ronan Le and Da, Jeff and Sakaguchi, Keisuke and Bosselut, Antoine and Choi, Yejin},
	month = oct,
	year = {2020},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/HETJTUCD/Hwang et al. - 2020 - COMET-ATOMIC 2020 On Symbolic and Neural Commonse.pdf:application/pdf;Snapshot:/Users/sharyarmemon/Zotero/storage/XPH63D2M/2010.html:text/html},
}

@article{sap_atomic_2018,
	title = {{ATOMIC}: {An} {Atlas} of {Machine} {Commonsense} for {If}-{Then} {Reasoning}},
	shorttitle = {{ATOMIC}},
	url = {https://arxiv.org/abs/1811.00146v3},
	doi = {10.48550/arXiv.1811.00146},
	abstract = {We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., "if X pays Y a compliment, then Y will likely return the compliment"). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.},
	language = {en},
	urldate = {2022-06-08},
	author = {Sap, Maarten and LeBras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A. and Choi, Yejin},
	month = oct,
	year = {2018},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/YQN7KZ9H/Sap et al. - 2018 - ATOMIC An Atlas of Machine Commonsense for If-The.pdf:application/pdf;Snapshot:/Users/sharyarmemon/Zotero/storage/T7Y9HV4Y/1811.html:text/html},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	shorttitle = {Transformers},
	url = {https://aclanthology.org/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2022-06-08},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	month = oct,
	year = {2020},
	pages = {38--45},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/Q3ACVWFN/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf:application/pdf},
}

@inproceedings{ehrlinger_towards_2016,
	title = {Towards a {Definition} of {Knowledge} {Graphs}},
	abstract = {Recently, the term knowledge graph has been used frequently in research and business, usually in close association with Semantic Web technologies, linked data, large-scale data analytics and cloud computing. Its popularity is clearly influenced by the introduction of Google's Knowledge Graph in 2012, and since then the term has been widely used without a definition. A large variety of interpretations has hampered the evolution of a common understanding of knowledge graphs. Numerous research papers refer to Google's Knowledge Graph, although no official documentation about the used methods exists. The prerequisite for widespread academic and commercial adoption of a concept or technology is a common understanding, based ideally on a definition that is free from ambiguity. We tackle this issue by discussing and defining the term knowledge graph, considering its history and diversity in interpretations and use. Our goal is to propose a definition of knowledge graphs that serves as basis for discussions on this topic and contributes to a common vision.},
	author = {Ehrlinger, Lisa and Wöß, Wolfram},
	month = sep,
	year = {2016},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/DL42QWUU/Ehrlinger and Wöß - 2016 - Towards a Definition of Knowledge Graphs.pdf:application/pdf},
}

@misc{noauthor_introducing_2012,
	title = {Introducing the {Knowledge} {Graph}: things, not strings},
	shorttitle = {Introducing the {Knowledge} {Graph}},
	url = {https://blog.google/products/search/introducing-knowledge-graph-things-not/},
	abstract = {We hope this will give you a more complete picture of your interest, provide smarter search results, and pique your curiosity.},
	language = {en-us},
	urldate = {2022-06-08},
	journal = {Google},
	month = may,
	year = {2012},
	file = {Snapshot:/Users/sharyarmemon/Zotero/storage/U7HJHIBY/introducing-knowledge-graph-things-not.html:text/html},
}

@article{farber_linked_2017,
	title = {Linked data quality of {DBpedia}, {Freebase}, {OpenCyc}, {Wikidata}, and {YAGO}},
	volume = {9},
	issn = {22104968, 15700844},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-170275},
	doi = {10.3233/SW-170275},
	abstract = {In recent years, several noteworthy large, cross-domain, and openly available knowledge graphs (KGs) have been created. These include DBpedia, Freebase, OpenCyc, Wikidata, and YAGO. Although extensively in use, these KGs have not been subject to an in-depth comparison so far. In this survey, we provide data quality criteria according to which KGs can be analyzed and analyze and compare the above mentioned KGs. Furthermore, we propose a framework for ﬁnding the most suitable KG for a given setting.},
	language = {en},
	number = {1},
	urldate = {2022-06-08},
	journal = {Semantic Web},
	author = {Färber, Michael and Bartscherer, Frederic and Menne, Carsten and Rettinger, Achim},
	editor = {Zaveri, Amrapali and Kontokostas, Dimitris and Hellmann, Sebastian and Umbrich, Jürgen and Zaveri, Amrapali and Kontokostas, Dimitris and Hellmann, Sebastian and Umbrich, Jürgen},
	month = nov,
	year = {2017},
	pages = {77--129},
	file = {Färber et al. - 2017 - Linked data quality of DBpedia, Freebase, OpenCyc,.pdf:/Users/sharyarmemon/Zotero/storage/PMRYAI6N/Färber et al. - 2017 - Linked data quality of DBpedia, Freebase, OpenCyc,.pdf:application/pdf},
}

@techreport{ilievski_cskg_2021,
	title = {{CSKG}: {The} {CommonSense} {Knowledge} {Graph}},
	shorttitle = {{CSKG}},
	url = {http://arxiv.org/abs/2012.11490},
	abstract = {Sources of commonsense knowledge support applications in natural language understanding, computer vision, and knowledge graphs. Given their complementarity, their integration is desired. Yet, their different foci, modeling approaches, and sparse overlap make integration difficult. In this paper, we consolidate commonsense knowledge by following five principles, which we apply to combine seven key sources into a first integrated CommonSense Knowledge Graph (CSKG). We analyze CSKG and its various text and graph embeddings, showing that CSKG is well-connected and that its embeddings provide a useful entry point to the graph. We demonstrate how CSKG can provide evidence for generalizable downstream reasoning and for pre-training of language models. CSKG and all its embeddings are made publicly available to support further research on commonsense knowledge integration and reasoning.},
	number = {arXiv:2012.11490},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Ilievski, Filip and Szekely, Pedro and Zhang, Bin},
	month = mar,
	year = {2021},
	doi = {10.48550/arXiv.2012.11490},
	note = {arXiv:2012.11490 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Social and Information Networks},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:2006.06114},
	file = {arXiv Fulltext PDF:/Users/sharyarmemon/Zotero/storage/532TLKIY/Ilievski et al. - 2021 - CSKG The CommonSense Knowledge Graph.pdf:application/pdf;arXiv.org Snapshot:/Users/sharyarmemon/Zotero/storage/WS4BSLB4/2012.html:text/html},
}

@techreport{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	number = {arXiv:2005.14165},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	doi = {10.48550/arXiv.2005.14165},
	note = {arXiv:2005.14165 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:/Users/sharyarmemon/Zotero/storage/ZXMZ5WPG/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/sharyarmemon/Zotero/storage/GI6KH8SD/2005.html:text/html},
}

@book{jurafsky_speech_2009,
	address = {USA},
	title = {Speech and {Language} {Processing} (2nd {Edition})},
	isbn = {978-0-13-187321-6},
	publisher = {Prentice-Hall, Inc.},
	author = {Jurafsky, Daniel and Martin, James H.},
	year = {2009},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	pages = {24},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/Users/sharyarmemon/Zotero/storage/MH6SD7TV/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@misc{noauthor_openai_nodate,
	title = {{OpenAI} {GPT2}},
	url = {https://huggingface.co/docs/transformers/model_doc/gpt2},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2022-06-09},
	file = {Snapshot:/Users/sharyarmemon/Zotero/storage/8VJRDY4V/gpt2.html:text/html},
}

@misc{noauthor_pretrained_nodate,
	title = {Pretrained models — transformers 2.2.0 documentation},
	url = {https://huggingface.co/transformers/v2.2.0/pretrained_models.html},
	urldate = {2022-06-09},
	file = {Pretrained models — transformers 2.2.0 documentation:/Users/sharyarmemon/Zotero/storage/YH4SDMGE/pretrained_models.html:text/html},
}

@misc{noauthor_gpt2-xl_nodate,
	title = {gpt2-xl · {Hugging} {Face}},
	url = {https://huggingface.co/gpt2-xl},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2022-06-09},
	file = {Snapshot:/Users/sharyarmemon/Zotero/storage/INFIFZ7B/gpt2-xl.html:text/html},
}

@misc{peterwestai2_symbolic_2022,
	title = {Symbolic {Knowledge} {Distillation}},
	url = {https://github.com/peterwestai2/symbolic-knowledge-distillation},
	urldate = {2022-06-09},
	author = {peterwestai2},
	month = jun,
	year = {2022},
	note = {original-date: 2021-10-13T02:08:39Z},
}

@article{wang_superglue_nodate,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁcult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	language = {en},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
	pages = {30},
	file = {Wang et al. - SuperGLUE A Stickier Benchmark for General-Purpos.pdf:/Users/sharyarmemon/Zotero/storage/QGYB9QJD/Wang et al. - SuperGLUE A Stickier Benchmark for General-Purpos.pdf:application/pdf},
}

@article{talmor_commonsenseqa_2018,
	title = {{CommonsenseQA}: {A} {Question} {Answering} {Challenge} {Targeting} {Commonsense} {Knowledge}},
	shorttitle = {{CommonsenseQA}},
	url = {https://arxiv.org/abs/1811.00937v2},
	doi = {10.48550/arXiv.1811.00937},
	abstract = {When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56\% accuracy, well below human performance, which is 89\%.},
	language = {en},
	urldate = {2022-06-14},
	author = {Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
	month = nov,
	year = {2018},
	file = {Full Text PDF:/Users/sharyarmemon/Zotero/storage/AWMN43KD/Talmor et al. - 2018 - CommonsenseQA A Question Answering Challenge Targ.pdf:application/pdf;Snapshot:/Users/sharyarmemon/Zotero/storage/V584K97A/1811.html:text/html},
}

@misc{rajbhandari_zero_2020,
	title = {{ZeRO}: {Memory} {Optimizations} {Toward} {Training} {Trillion} {Parameter} {Models}},
	shorttitle = {{ZeRO}},
	url = {http://arxiv.org/abs/1910.02054},
	doi = {10.48550/arXiv.1910.02054},
	abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.},
	urldate = {2022-08-05},
	publisher = {arXiv},
	author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
	month = may,
	year = {2020},
	note = {Number: arXiv:1910.02054
arXiv:1910.02054 [cs, stat]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sharyarmemon/Zotero/storage/9GX5WFTQ/Rajbhandari et al. - 2020 - ZeRO Memory Optimizations Toward Training Trillio.pdf:application/pdf;arXiv.org Snapshot:/Users/sharyarmemon/Zotero/storage/WKCMRXUU/1910.html:text/html},
}

@misc{noauthor_kelm_2021,
	title = {{KELM}: {Integrating} {Knowledge} {Graphs} with {Language} {Model} {Pre}-training {Corpora}},
	shorttitle = {{KELM}},
	url = {https://ai.googleblog.com/2021/05/kelm-integrating-knowledge-graphs-with.html},
	language = {en},
	urldate = {2023-07-13},
	month = may,
	year = {2021},
	file = {Snapshot:/Users/sharyarmemon/Zotero/storage/8TG2ZYHC/kelm-integrating-knowledge-graphs-with.html:text/html},
}

@misc{speer2018conceptnet,
      title={ConceptNet 5.5: An Open Multilingual Graph of General Knowledge}, 
      author={Robyn Speer and Joshua Chin and Catherine Havasi},
      year={2018},
      eprint={1612.03975},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2023-09-10},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
	annote = {Comment: 12 pages, 3 figures, NeurIPS 2019},
	file = {arXiv Fulltext PDF:/Users/sharyarmemon/Zotero/storage/H2ESQAG5/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/sharyarmemon/Zotero/storage/4DTBEEEQ/1912.html:text/html},
}
